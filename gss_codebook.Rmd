---
title: "Parsing the GSS Codebook"
author: "Kieran healy"
date: "10/6/2019"
output: html_document
---

# Setup

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      fig.showtext=TRUE)
```

```{r libraries, message = FALSE}
library(tidyverse)
library(rvest)
library(socviz)
```

```{r theme, message = FALSE}
library(showtext)
showtext_auto()
library(myriad)
import_myriad_semi()

theme_set(theme_myriad_semi())
```

```{r, functions}

## Page of variables to list of variables and their info
parse_page <- function(x){
  html_nodes(x, ".dflt") %>%
    map(~ html_nodes(.x, ".noborder")) %>%
    map(~ html_table(.x))
}

## Length of each list element
## Standard GSS Qs will have 4 elements
## Ids recodes and other things will have 3
get_lengths <- function(x){
  map(x, length)
}

get_names <- function(x){
  map(x, names)
}

## Variable short names and descriptions
get_var_ids <- function(x){
  x %>% map_dfr(1) %>%
    select(id = X1, description = X3) %>%
    as_tibble()
}


## Question Text
get_text <- function(x, y){
  if(y[[1]] == 3) {
    return(NA_character_)
  } else {
    stringr::str_trim(x[[2]])
  }
}

## Question Marginals
get_marginals <- function(x, y){
  if(y[[1]] == 3) {
    tmp <- x[[2]]
  } else {
    tmp <- x[[3]]
  }
  
  if(ncol(tmp) == 2) {
    as_tibble(tmp) %>%
      select(cases = X1, range = X2)
  } else {
    tmp <- as_tibble(tmp[, colSums(is.na(tmp)) != nrow(tmp)]) %>%
      janitor::clean_names()
    tmp$value <- as.character(tmp$value)
    tmp
  }
}

## Add an id column
add_id <- function(x, y){
  x %>% add_column(id = y)
}

## Question Properties
get_props <- function(x, y){
  if(y[[1]] == 3) {
    tmp <- x[[3]]
    colnames(tmp) <- c("property", "value")
    tmp <- as_tibble(tmp)
    tmp$property <- stringr::str_remove(tmp$property, ":")
    tmp
  } else {
    tmp <- x[[4]]
    colnames(tmp) <- c("property", "value")
    tmp <- as_tibble(tmp)
    tmp$property <- stringr::str_remove(tmp$property, ":")
    tmp
  }
}

## Take the functions above and process a page to a tibble of cleaned records

process_page <- function(x){
  page <- parse_page(x)
  q_vars <- get_var_ids(page)
  lens <- get_lengths(page)
  keys <- q_vars$id
  
  q_text <- map2_chr(page, lens, ~ get_text(.x, .y))
  q_text <- stringr::str_trim(q_text)
  q_text <- stringr::str_remove_all(q_text, "\n")
  q_text <- tibble(id = keys, q_text = q_text)
  q_text <- q_text %>%
    mutate(q_text = replace_na(q_text, "None"))
  q_marginals <- map2(page, lens, ~ get_marginals(.x, .y)) %>%
    set_names(keys) 
  q_marginals <- map2(q_marginals, keys, ~ add_id(.x, .y))
  
  q_props <- map2(page, lens, ~ get_props(.x, .y)) %>%
    set_names(keys) 
  q_props <- map2(q_props, keys, ~ add_id(.x, .y))
  
  q_tbl <- q_vars %>% 
    add_column(properties = q_props) %>% 
    add_column(marginals = q_marginals) %>%
    left_join(q_text) %>%
    rename(text = q_text)
  
  q_tbl

  }

```


# GSS Cumulative Data File

## Initial data scraping and saving to local storage

This next code chunk shows how we got the codebook data, but it is not evaluated here, because we only need to do it once.

```{r, eval = FALSE, echo = TRUE}

## Generate vector of doc page urls
urls <- paste0("https://sda.berkeley.edu/D3/GSS18/Doc/", 
               "hcbk", sprintf('%0.4d', 1:261), ".htm")


## Grab the codebook pages one at a time
doc_pages <- urls %>% 
  map(~ {
    message(glue::glue("* parsing: {.x}"))
    Sys.sleep(5) # try to be polite
    safely(read_html)(.x)
  })

```

## Save the scraped webpages locally

There's a gotcha with objects like `doc_pages`: they cannot be straightforwardly saved to R's native data format with `save()`. The XML files are stored with external pointers to their content and cannot be "serialized" in a way that saves their content properly. If you try, when you `load()` the saved object you will get complaints about missing pointers. So instead, we'll unspool our list and save each fellow's page individually. Then if we want to rerun this analysis without crawling everything again, we will load them in from our local saved versions using `read_html()`.

Again, this code chunk is shown but not run, as we only do it once. 

```{r localsave, eval = FALSE, echo = TRUE}

## Get a list containing every fellow's webpage, 
## Drop the safely() error codes from the initial scrape, and 
## and also drop any NULL entries
page_list <- pluck(doc_pages, "result") %>% 
  compact()

## Make a vector of clean file names of the form "raw/001.htm"
## One for every fellow. Same order as the page_list.
fnames <-paste0("raw/cumulative/", 
                sprintf('%0.4d', 1:261),
                ".htm") 

## Walk the elements of the page list and the file names to 
## save each HTML file under is respective local file name
walk2(page_list, fnames, ~ write_xml(.x, file = .y))

```

# Parse the pages 

Using the local data we've saved, we read in a list of all the web pages. Our goal is to get them into a tractable format (a tibble or data frame). From there we can write some functions to, e.g., query the codebook directly from the console, or alterantively produce the codebook in a format suitable for integrating into the R help system via a package.

```{r localparse}

## The names of all the files we just created
local_urls <- fs::dir_ls("raw/cumulative/")

## Read all the pages back in, from local storage 
doc_pages <- local_urls %>% 
  map(~ {
    safely(read_html)(.x)
  })

## Are there any errors?
doc_pages %>% pluck("error") %>% 
  flatten_dfr()

## quick look at first five items in the list
summary(doc_pages)[1:5,]

## Quick look inside the first record
doc_pages[[1]]

```

Next, we parse every webpage to extract a row for every variable. There are multiple variables per page. Parse the GSS variables into a tibble, with list columns for the marginals and the variable properties.

```{r}
panel_doc <-  doc_pages %>% 
  pluck("result") %>% # Get just the webpages
  compact() %>%
  map(process_page) %>%
  bind_rows()
```


```{r}
gss_doc %>% 
  filter(id == "RACE") %>% 
  select(marginals) %>% 
  unnest(cols = c(marginals))
```

```{r}
gss_doc %>% 
  filter(id == "WTSSALL") %>% 
  select(marginals) %>% 
  unnest(cols = c(marginals))
```

```{r}
gss_doc %>% 
  filter(id == "WTSSALL") %>% 
  select(properties) %>% 
  unnest(cols = c(properties))
```

## Write out the data in an Rd-useful format

```{r, eval = FALSE}

outfile <- file("rdoc/roxygen.txt")
writeLines(
  paste("#' \\item{\\code{", gss_doc$id,"}}{", 
       gss_doc$description, 
       " Further information: ", 
      gss_doc$text, "}"), 
  outfile
)
close(outfile)

```


## Save the data object

```{r}
save(gss_doc, file = "data/gss_doc.rda", compress = "xz")
# tools::resaveRdaFiles("data")
tools::checkRdaFiles("data")
```


# GSS Panel Data: Same deal

## Initial data scraping and saving to local storage

This next code chunk shows how we got the codebook data, but it is not evaluated here, because we only need to do it once.

```{r, eval = FALSE, echo = TRUE}

## Generate vector of doc page urls
urls <- paste0("https://sda.berkeley.edu/D3/GSS06panelw3/Doc/", 
               "hcbk", sprintf('%0.4d', 1:78), ".htm")


## Grab the codebook pages one at a time
panel_pages <- urls %>% 
  map(~ {
    message(glue::glue("* parsing: {.x}"))
    Sys.sleep(5) # try to be polite
    safely(read_html)(.x)
  })

```

## Save the scraped webpages locally

Again, this code chunk is shown but not run, as we only do it once. 

```{r localsave, eval = FALSE, echo = TRUE}

## Get a list containing every fellow's webpage, 
## Drop the safely() error codes from the initial scrape, and 
## and also drop any NULL entries
panel_page_list <- pluck(panel_pages, "result") %>% 
  compact()

## Make a vector of clean file names of the form "raw/001.htm"
## One for every fellow. Same order as the page_list.
panel_fnames <-paste0("raw/panel/", 
                sprintf('%0.4d', 1:78),
                ".htm") 

## Walk the elements of the page list and the file names to 
## save each HTML file under is respective local file name
walk2(panel_page_list, panel_fnames, ~ write_xml(.x, file = .y))

```

## Parse the Panel pages 

Using the local data we've saved, we read in a list of all the web pages. Our goal is to get them into a tractable format (a tibble or data frame). From there we can write some functions to, e.g., query the codebook directly from the console, or alterantively produce the codebook in a format suitable for integrating into the R help system via a package.

```{r localparse}

## The names of all the files we just created
panel_local_urls <- fs::dir_ls("raw/panel/")

## Read all the pages back in, from local storage 
panel_pages <- panel_local_urls %>% 
  map(~ {
    safely(read_html)(.x)
  })

## Are there any errors?
panel_pages %>% pluck("error") %>% 
  flatten_dfr()

## quick look at first five items in the list
summary(panel_pages)[1:5,]

## Quick look inside the first record
panel_pages[[1]]

```

Next, we parse every webpage to extract a row for every variable. There are multiple variables per page. Parse the GSS panel variables into a tibble, with list columns for the marginals and the variable properties.

```{r}
gss_panel_doc <-  panel_pages %>% 
  pluck("result") %>% # Get just the webpages
  compact() %>%
  map(process_page) %>%
  bind_rows()
```

```{r}
gss_panel_doc %>% 
  filter(id == "RACE_1") %>% 
  select(marginals) %>% 
  unnest(cols = c(marginals))
```

## Write out the data in an Rd-useful format

```{r, eval = FALSE}

outfile <- file("rdoc/panel-roxygen.txt")
writeLines(
  paste("#' \\item{\\code{", gss_panel_doc$id,"}}{", 
       gss_panel_doc$description, 
       " Further information: ", 
      gss_panel_doc$text, "}"), 
  outfile
)
close(outfile)

```


## Save the data object

```{r}
save(gss_panel_doc, file = "data/gss_panel_doc.rda", compress = "xz")
# tools::resaveRdaFiles("data")
tools::checkRdaFiles("data")
```




